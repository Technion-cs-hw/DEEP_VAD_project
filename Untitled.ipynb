{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a16821-ffce-4ee0-ab5a-fb11ca0d7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da5c19-792e-4e5f-bbd7-eb96f7b110b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available(): \n",
    "     dev = \"cuda:0\" \n",
    "    else: \n",
    "     dev = \"cpu\" \n",
    "    return torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece85ca-24d9-4bbb-bbbd-0b4baf6843ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = utils.create_dataloaders()\n",
    "\n",
    "# Define hyperparameters\n",
    "latent_dim = 64  # Size of latent vectors\n",
    "image_dim = 28*28  # Size of flattened images\n",
    "\n",
    "model = AutoDecoder(in_channels=latent_dim, out_channels=image_dim, train_ds_len=len(train_ds))\n",
    "optimizer = torch.optim.Adam(model.parameters() , lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "device = get_device()\n",
    "\n",
    "trainer = ADTrainer(model,loss_fn,optimizer,device)\n",
    "\n",
    "num_epochs = 50\n",
    "checkpoint_file = None\n",
    "checkpoint_file_final = f'{checkpoint_file}_final'\n",
    "early_stopping = None\n",
    "print_every = 1\n",
    "post_epoch_fn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d26df-b08b-4e45-a8aa-511467268856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "\n",
    "def post_epoch_fn(epoch, train_result, test_result, verbose):\n",
    "    # Plot some samples if this is a verbose epoch\n",
    "    if verbose:\n",
    "        samples = vae.sample(n=5)\n",
    "        fig, _ = plot.tensors_as_images(samples, figsize=(6,2))\n",
    "        IPython.display.display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "if os.path.isfile(f'{checkpoint_file_final}.pt'):\n",
    "    print(f'*** Loading final checkpoint file {checkpoint_file_final} instead of training')\n",
    "    checkpoint_file = checkpoint_file_final\n",
    "else:\n",
    "    res = trainer.fit(dl_train, dl_test,\n",
    "                      num_epochs, early_stopping, print_every,\n",
    "                      checkpoint_file,post_epoch_fn)\n",
    "    \n",
    "# Plot images from best model\n",
    "saved_state = torch.load(f'{checkpoint_file}.pt', map_location=device)\n",
    "vae_dp.load_state_dict(saved_state['model_state'])\n",
    "print('*** Images Generated from best model:')\n",
    "fig, _ = plot.tensors_as_images(vae_dp.module.sample(n=15), nrows=3, figsize=(6,6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
