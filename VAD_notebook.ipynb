{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fd762c-dc28-4f62-9c7f-009635fbf578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Beta\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import utils\n",
    "import evaluate\n",
    "import cs236781.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39bd7ee-45ba-41fd-8d5e-4cf46b0aea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO_loss(recon_x, x, mu, logvar, beta=1, b1 = 255, a1 = 0, distr='normal'):\n",
    "    recon_error = nn.functional.mse_loss(recon_x, x, reduction='sum') / x.size(0)\n",
    "    #recon_error = evaluate.reconstruction_loss(recon_x,x)\n",
    "    #recon_error = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum') / x.size(0)\n",
    "\n",
    "    if distr == \"normal\":\n",
    "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), axis=1) / x.size(0)\n",
    "    elif distr == \"lognormal\":\n",
    "        kl = 0.5 * (mu.pow(2) + logvar.exp() - 1 - logvar) / x.size(0)\n",
    "    elif distr == \"uniform\":\n",
    "        #Assuming mu, logvar = b2,a2 represent the learned bounds\n",
    "        kl = torch.log((mu - logvar) / (b1 - a1))\n",
    "    else:\n",
    "        raise ValueError(f\"Distribution {distr} not recognized.\")\n",
    "\n",
    "    return recon_error + beta * kl.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad601f1-31e5-4ab1-9957-cbe5056a6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo2_loss(x, x_rec, mu, sigma):\n",
    "    # Reconstruction loss summed over elements and averaged over batch\n",
    "    reconstruction_loss = torch.nn.functional.mse_loss(x_rec, x, reduction='sum') / x.size(0)\n",
    "    \n",
    "    # KL divergence per sample summed over latent dimensions\n",
    "    kl_divergence = -0.5 * torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2), dim=1)\n",
    "    kl_divergence = kl_divergence.mean()\n",
    "\n",
    "    elbo = reconstruction_loss + kl_divergence\n",
    "    \n",
    "    return elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ddf762-97e3-4169-ae4f-2aa6018a1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensors_as_images(images):\n",
    "    fig, axs = plt.subplots(1, len(images), figsize=(15, 3))  # Create 1x5 grid of subplots\n",
    "    for i in range(5):\n",
    "        axs[i].imshow(images[i].cpu().detach().numpy(), cmap='Grays')  # Convert tensor to numpy for plotting\n",
    "        axs[i].axis('off')  # Hide axes\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c76674b-de4c-4e61-839e-72fe3337925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available(): \n",
    "     dev = \"cuda:0\" \n",
    "    else: \n",
    "     dev = \"cpu\" \n",
    "    return torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d291a15-d046-448c-b4bf-4817f4452d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f0b18d03a60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(device)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad5b3f2-da1e-4175-b7c8-00c376e9be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoDecoder(nn.Module):\n",
    "    def __init__(self, x_dim, z_dim, mu, sigma, distr = \"normal\", device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.x_dim = x_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.distr = distr\n",
    "        self.mu = nn.parameter.Parameter(mu,True) \n",
    "        self.sigma = nn.parameter.Parameter(sigma,True) \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, 128, kernel_size=7, stride=1, padding=0),  \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Output in range [0, 1]\n",
    "        )\n",
    "        \n",
    "        self.mlp =  nn.Linear(28*28,28*28,bias=True)\n",
    "        \n",
    "    def sample_vectors(self, mu, sigma):\n",
    "        Z = torch.randn_like(mu, requires_grad=True).to(self.device)\n",
    "        X = sigma*Z + mu\n",
    "        return X\n",
    "        \n",
    "    def forward(self, mu, sigma):        \n",
    "        z = self.sample_vectors(mu, sigma)\n",
    "        z = z.view(-1, self.z_dim, 1, 1)\n",
    "        print(z.shape)\n",
    "        print(self.decoder(z).shape)\n",
    "        reconstructed_images = 255 * self.decoder(z).view(-1,28*28) # Output in range [0, 255]\n",
    "        reconstructed_images = self.mlp(reconstructed_images)\n",
    "        return reconstructed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a26a388-12d3-4cba-a198-862af0d7fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 500\n",
    "X_DIM = 28 * 28  \n",
    "Z_DIM = 100\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "config = {\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"LEARNING_RATE\": LEARNING_RATE,\n",
    "    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "    \"X_DIM\": X_DIM,\n",
    "    \"Z_DIM\": Z_DIM,\n",
    "    \"NUM_CLASSES\": NUM_CLASSES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bec8be9-6b3e-4b48-b0a7-2247497e2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = utils.create_dataloaders(\"dataset\", device, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2062ac8-60c8-4cf1-9964-2276292a08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.randn(NUM_CLASSES, Z_DIM, requires_grad=True)\n",
    "sigma = torch.randn(NUM_CLASSES, Z_DIM, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83da0e9d-301d-43a1-85c8-eac037f9f7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoDecoder(\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(100, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      "  (mlp): Linear(in_features=784, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vad = VariationalAutoDecoder(x_dim=X_DIM, \n",
    "                             z_dim=Z_DIM, \n",
    "                             mu=mu, \n",
    "                             sigma=sigma, \n",
    "                             device=device).to(device)\n",
    "print(vad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b106f3-817b-4ecf-9605-e16f2eb73712",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(params=vad.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5f5cbb2-e9f8-444d-88d0-f44fd45177c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING with  {'BATCH_SIZE': 64, 'LEARNING_RATE': 0.001, 'NUM_EPOCHS': 500, 'X_DIM': 784, 'Z_DIM': 100, 'NUM_CLASSES': 10}\n",
      "torch.Size([64, 100, 1, 1])\n",
      "torch.Size([64, 1, 8, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 784]' is invalid for input of size 4096",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m mu_batch \u001b[38;5;241m=\u001b[39m vad\u001b[38;5;241m.\u001b[39mmu[labels]\n\u001b[1;32m     10\u001b[0m sigma_batch \u001b[38;5;241m=\u001b[39m vad\u001b[38;5;241m.\u001b[39msigma[labels] \n\u001b[0;32m---> 12\u001b[0m x_reconstruction \u001b[38;5;241m=\u001b[39m \u001b[43mvad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m x_reconstruction \u001b[38;5;241m=\u001b[39m x_reconstruction\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m elbo2_loss(x_reconstruction, x\u001b[38;5;241m.\u001b[39mfloat(), mu_batch, sigma_batch)\n",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m, in \u001b[0;36mVariationalAutoDecoder.forward\u001b[0;34m(self, mu, sigma)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 37\u001b[0m reconstructed_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Output in range [0, 255]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m reconstructed_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(reconstructed_images)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reconstructed_images\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 784]' is invalid for input of size 4096"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "print(\"TRAINING with \", config)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    batch_losses = []\n",
    "    \n",
    "    for batch_i, batch in enumerate(train_dl):\n",
    "        idx, x = batch\n",
    "        labels = train_ds.y[idx]\n",
    "        mu_batch = vad.mu[labels]\n",
    "        sigma_batch = vad.sigma[labels] \n",
    "        \n",
    "        x_reconstruction = vad.forward(mu_batch, sigma_batch)\n",
    "        x_reconstruction = x_reconstruction.view(x.size(0), 28, 28)\n",
    "        \n",
    "        loss = elbo2_loss(x_reconstruction, x.float(), mu_batch, sigma_batch)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        batch_losses.append(loss.data.cpu().item())\n",
    "    \n",
    "    train_losses.append(np.mean(batch_losses))\n",
    "\n",
    "    if(epoch % 50 == 0):\n",
    "        print(\"epoch: {} training loss: {:.5f}\".format(epoch, train_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c384930-27e6-4ac4-accd-59c7a20d4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = random.sample(range(1000), 5)\n",
    "distr_params = vad.distr_params[random_indices]\n",
    "initial = train_ds[random_indices][1]\n",
    "\n",
    "x = vad(distr_params)\n",
    "restored = x.view(-1, 28, 28)\n",
    "\n",
    "plot_tensors_as_images(restored)\n",
    "plot_tensors_as_images(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead2da4-cca8-4ca4-8b4a-6f89cabf2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vectors = vad.sample_vectors(vad.distr_params)\n",
    "utils.plot_tsne(train_ds, latent_vectors, \"VAD_TSNE_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005de1cd-e776-4570-8bc8-d98f30fe3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test_latents = torch.randn(len(test_ds), Z_DIM, requires_grad=True, device = device)\n",
    "vad.latent_vectors = nn.parameter.Parameter(test_latents,True)\n",
    "opt = torch.optim.Adam([vad.latent_vectors], lr=LEARNING_RATE)\n",
    "\n",
    "evaluate.evaluate_model(vad, test_dl, opt, vad.latent_vectors, 50 , device)\n",
    "utils.plot_tsne(train_ds, vad.latent_vectors, \"VAD_TSNE_test\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
