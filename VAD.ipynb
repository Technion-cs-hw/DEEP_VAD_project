{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a16821-ffce-4ee0-ab5a-fb11ca0d7364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danyil.vaida/miniconda3/envs/cs236781-project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "import training\n",
    "import AutoDecoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5da5c19-792e-4e5f-bbd7-eb96f7b110b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available(): \n",
    "     dev = \"cuda:0\" \n",
    "    else: \n",
    "     dev = \"cpu\" \n",
    "    return torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9a68be-b23f-43c5-9613-b963a11cb61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aece85ca-24d9-4bbb-bbbd-0b4baf6843ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = utils.create_dataloaders(\"dataset\")\n",
    "\n",
    "# Define hyperparameters\n",
    "latent_dim = 64  # Size of latent vectors\n",
    "image_dim = 28*28  # Size of flattened images\n",
    "\n",
    "model = AutoDecoder.AutoDecoder(in_channels=latent_dim, out_channels=image_dim, train_ds_len=len(train_ds), device = device)\n",
    "optimizer = torch.optim.Adam(model.parameters() , lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "trainer = training.ADTrainer(model,loss_fn,optimizer,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3f4c9-3c0c-4e07-8f32-acf75289c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "checkpoint_file = None\n",
    "checkpoint_file_final = f'{checkpoint_file}_final'\n",
    "early_stopping = None\n",
    "print_every = 1\n",
    "post_epoch_fn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222d26df-b08b-4e45-a8aa-511467268856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 1/50 ---\n",
      "train_batch:   0%|                                                                             | 0/1000 [00:00<?, ?it/s][tensor([0]), tensor([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   4,   0,   0,   0,   0,   0,  62,  61,  21,  29,\n",
      "           23,  51, 136,  61,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,  88, 201, 228, 225, 255, 115,  62,\n",
      "          137, 255, 235, 222, 255, 135,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,  47, 252, 234, 238, 224, 215, 215, 229, 108,\n",
      "          180, 207, 214, 224, 231, 249, 254,  45,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   1,   0,   0, 214, 222, 210, 213, 224, 225, 217, 220, 254,\n",
      "          233, 219, 221, 217, 223, 221, 240, 254,   0,   0,   1,   0,   0,   0],\n",
      "         [  1,   0,   0,   0, 128, 237, 207, 224, 224, 207, 216, 214, 210, 208,\n",
      "          211, 221, 208, 219, 213, 226, 211, 237, 150,   0,   0,   0,   0,   0],\n",
      "         [  0,   2,   0,   0, 237, 222, 215, 207, 210, 212, 213, 206, 214, 213,\n",
      "          214, 213, 210, 215, 214, 206, 199, 218, 255,  13,   0,   2,   0,   0],\n",
      "         [  0,   4,   0,  85, 228, 210, 218, 200, 211, 208, 203, 215, 210, 209,\n",
      "          209, 210, 213, 211, 210, 217, 206, 213, 231, 175,   0,   0,   0,   0],\n",
      "         [  0,   0,   0, 217, 224, 215, 206, 205, 204, 217, 230, 222, 215, 224,\n",
      "          233, 228, 232, 228, 224, 207, 212, 215, 213, 229,  31,   0,   4,   0],\n",
      "         [  1,   0,  21, 225, 212, 212, 203, 211, 225, 193, 139, 136, 195, 147,\n",
      "          156, 139, 128, 162, 197, 223, 207, 220, 213, 232, 177,   0,   0,   0],\n",
      "         [  0,   0, 123, 226, 207, 211, 209, 205, 228, 158,  90, 103, 186, 138,\n",
      "          100, 121, 147, 158, 183, 226, 208, 214, 209, 216, 255,  13,   0,   1],\n",
      "         [  0,   0, 226, 219, 202, 208, 206, 205, 216, 184, 156, 150, 193, 170,\n",
      "          164, 168, 188, 186, 200, 219, 216, 213, 213, 211, 233, 148,   0,   0],\n",
      "         [  0,  45, 227, 204, 214, 211, 218, 222, 221, 230, 229, 221, 213, 224,\n",
      "          233, 226, 220, 219, 221, 224, 223, 217, 210, 218, 213, 254,   0,   0],\n",
      "         [  0, 157, 226, 203, 207, 211, 209, 215, 205, 198, 207, 208, 201, 201,\n",
      "          197, 203, 205, 210, 207, 213, 214, 214, 214, 213, 208, 234, 107,   0],\n",
      "         [  0, 235, 213, 204, 211, 210, 209, 213, 202, 197, 204, 215, 217, 213,\n",
      "          212, 210, 206, 212, 203, 211, 218, 215, 214, 208, 209, 222, 230,   0],\n",
      "         [ 52, 255, 207, 200, 208, 213, 210, 210, 208, 207, 202, 201, 209, 216,\n",
      "          216, 216, 216, 214, 212, 205, 215, 201, 228, 208, 214, 212, 218,  25],\n",
      "         [118, 217, 201, 206, 208, 213, 208, 205, 206, 210, 211, 202, 199, 207,\n",
      "          208, 209, 210, 207, 210, 210, 245, 139, 119, 255, 202, 203, 236, 114],\n",
      "         [171, 238, 212, 203, 220, 216, 217, 209, 207, 205, 210, 211, 206, 204,\n",
      "          206, 209, 211, 215, 210, 206, 221, 242,   0, 224, 234, 230, 181,  26],\n",
      "         [ 39, 145, 201, 255, 157, 115, 250, 200, 207, 206, 207, 213, 216, 206,\n",
      "          205, 206, 207, 206, 215, 207, 221, 238,   0,   0, 188,  85,   0,   0],\n",
      "         [  0,   0,   0,  31,   0, 129, 253, 190, 207, 208, 208, 208, 209, 211,\n",
      "          211, 209, 209, 209, 212, 201, 226, 165,   0,   0,   0,   0,   0,   0],\n",
      "         [  2,   0,   0,   0,   0,  89, 254, 199, 199, 192, 196, 198, 199, 201,\n",
      "          202, 203, 204, 203, 203, 200, 222, 155,   0,   3,   3,   3,   2,   0],\n",
      "         [  0,   0,   1,   5,   0,   0, 255, 218, 226, 232, 228, 224, 222, 220,\n",
      "          219, 219, 217, 221, 220, 212, 236,  95,   0,   2,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0, 155, 194, 168, 170, 171, 173, 173, 179,\n",
      "          177, 175, 172, 171, 167, 161, 180,   0,   0,   1,   0,   1,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]])]\n",
      "HERE\n",
      "tensor([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   4,   0,   0,   0,   0,   0,  62,  61,  21,  29,\n",
      "           23,  51, 136,  61,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,  88, 201, 228, 225, 255, 115,  62,\n",
      "          137, 255, 235, 222, 255, 135,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,  47, 252, 234, 238, 224, 215, 215, 229, 108,\n",
      "          180, 207, 214, 224, 231, 249, 254,  45,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   1,   0,   0, 214, 222, 210, 213, 224, 225, 217, 220, 254,\n",
      "          233, 219, 221, 217, 223, 221, 240, 254,   0,   0,   1,   0,   0,   0],\n",
      "         [  1,   0,   0,   0, 128, 237, 207, 224, 224, 207, 216, 214, 210, 208,\n",
      "          211, 221, 208, 219, 213, 226, 211, 237, 150,   0,   0,   0,   0,   0],\n",
      "         [  0,   2,   0,   0, 237, 222, 215, 207, 210, 212, 213, 206, 214, 213,\n",
      "          214, 213, 210, 215, 214, 206, 199, 218, 255,  13,   0,   2,   0,   0],\n",
      "         [  0,   4,   0,  85, 228, 210, 218, 200, 211, 208, 203, 215, 210, 209,\n",
      "          209, 210, 213, 211, 210, 217, 206, 213, 231, 175,   0,   0,   0,   0],\n",
      "         [  0,   0,   0, 217, 224, 215, 206, 205, 204, 217, 230, 222, 215, 224,\n",
      "          233, 228, 232, 228, 224, 207, 212, 215, 213, 229,  31,   0,   4,   0],\n",
      "         [  1,   0,  21, 225, 212, 212, 203, 211, 225, 193, 139, 136, 195, 147,\n",
      "          156, 139, 128, 162, 197, 223, 207, 220, 213, 232, 177,   0,   0,   0],\n",
      "         [  0,   0, 123, 226, 207, 211, 209, 205, 228, 158,  90, 103, 186, 138,\n",
      "          100, 121, 147, 158, 183, 226, 208, 214, 209, 216, 255,  13,   0,   1],\n",
      "         [  0,   0, 226, 219, 202, 208, 206, 205, 216, 184, 156, 150, 193, 170,\n",
      "          164, 168, 188, 186, 200, 219, 216, 213, 213, 211, 233, 148,   0,   0],\n",
      "         [  0,  45, 227, 204, 214, 211, 218, 222, 221, 230, 229, 221, 213, 224,\n",
      "          233, 226, 220, 219, 221, 224, 223, 217, 210, 218, 213, 254,   0,   0],\n",
      "         [  0, 157, 226, 203, 207, 211, 209, 215, 205, 198, 207, 208, 201, 201,\n",
      "          197, 203, 205, 210, 207, 213, 214, 214, 214, 213, 208, 234, 107,   0],\n",
      "         [  0, 235, 213, 204, 211, 210, 209, 213, 202, 197, 204, 215, 217, 213,\n",
      "          212, 210, 206, 212, 203, 211, 218, 215, 214, 208, 209, 222, 230,   0],\n",
      "         [ 52, 255, 207, 200, 208, 213, 210, 210, 208, 207, 202, 201, 209, 216,\n",
      "          216, 216, 216, 214, 212, 205, 215, 201, 228, 208, 214, 212, 218,  25],\n",
      "         [118, 217, 201, 206, 208, 213, 208, 205, 206, 210, 211, 202, 199, 207,\n",
      "          208, 209, 210, 207, 210, 210, 245, 139, 119, 255, 202, 203, 236, 114],\n",
      "         [171, 238, 212, 203, 220, 216, 217, 209, 207, 205, 210, 211, 206, 204,\n",
      "          206, 209, 211, 215, 210, 206, 221, 242,   0, 224, 234, 230, 181,  26],\n",
      "         [ 39, 145, 201, 255, 157, 115, 250, 200, 207, 206, 207, 213, 216, 206,\n",
      "          205, 206, 207, 206, 215, 207, 221, 238,   0,   0, 188,  85,   0,   0],\n",
      "         [  0,   0,   0,  31,   0, 129, 253, 190, 207, 208, 208, 208, 209, 211,\n",
      "          211, 209, 209, 209, 212, 201, 226, 165,   0,   0,   0,   0,   0,   0],\n",
      "         [  2,   0,   0,   0,   0,  89, 254, 199, 199, 192, 196, 198, 199, 201,\n",
      "          202, 203, 204, 203, 203, 200, 222, 155,   0,   3,   3,   3,   2,   0],\n",
      "         [  0,   0,   1,   5,   0,   0, 255, 218, 226, 232, 228, 224, 222, 220,\n",
      "          219, 219, 217, 221, 220, 212, 236,  95,   0,   2,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0, 155, 194, 168, 170, 171, 173, 173, 179,\n",
      "          177, 175, 172, 171, 167, 161, 180,   0,   0,   1,   0,   1,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]])\n",
      "train_batch:   0%|                                                                             | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     checkpoint_file \u001b[38;5;241m=\u001b[39m checkpoint_file_final\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcheckpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpost_epoch_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_epoch_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Plot images from best model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m saved_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/deep/project/training.py:86\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, dl_train, dl_test, num_epochs, checkpoints, early_stopping, print_every, post_epoch_fn, **kw)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# TODO: Train & evaluate for one epoch\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#  - Use the train/test_epoch methods.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#  - Save losses and accuracies in the lists above.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# ====== YOUR CODE: ======\u001b[39;00m\n\u001b[1;32m     85\u001b[0m actual_num_epochs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 86\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_epoch(dl_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m     88\u001b[0m train_acc\u001b[38;5;241m.\u001b[39mappend(train_result\u001b[38;5;241m.\u001b[39maccuracy)\n",
      "File \u001b[0;32m~/deep/project/training.py:133\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, dl_train, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03mTrain once over a training set (single epoch).\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m:param dl_train: DataLoader for the training set.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m:param kw: Keyword args supported by _foreach_batch.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m:return: An EpochResult for the epoch.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# set train mode\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep/project/training.py:209\u001b[0m, in \u001b[0;36mTrainer._foreach_batch\u001b[0;34m(dl, forward_fn, verbose, max_batches)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m    208\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dl_iter)\n\u001b[0;32m--> 209\u001b[0m     batch_res \u001b[38;5;241m=\u001b[39m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpbar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_res\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/deep/project/training.py:259\u001b[0m, in \u001b[0;36mADTrainer.train_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m latent \u001b[38;5;241m=\u001b[39m latent\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Image batch (N,C,H,W)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# ====== YOUR CODE: ======\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(images, reconstructed)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/deep/project/AutoDecoder.py:32\u001b[0m, in \u001b[0;36mAutoDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-project/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-project/lib/python3.10/site-packages/torch/nn/modules/conv.py:948\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    943\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    944\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    946\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [1]"
     ]
    }
   ],
   "source": [
    "import IPython.display\n",
    "\n",
    "if os.path.isfile(f'{checkpoint_file_final}.pt'):\n",
    "    print(f'*** Loading final checkpoint file {checkpoint_file_final} instead of training')\n",
    "    checkpoint_file = checkpoint_file_final\n",
    "else:\n",
    "    res = trainer.fit(train_dl, test_dl,\n",
    "                      num_epochs=num_epochs, early_stopping=early_stopping, print_every=print_every,\n",
    "                      checkpoints=checkpoint_file,post_epoch_fn=post_epoch_fn)\n",
    "    \n",
    "# Plot images from best model\n",
    "saved_state = torch.load(f'{checkpoint_file}.pt', map_location=device)\n",
    "vae_dp.load_state_dict(saved_state['model_state'])\n",
    "print('*** Images Generated from best model:')\n",
    "fig, _ = plot.tensors_as_images(vae_dp.module.sample(n=15), nrows=3, figsize=(6,6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
